{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOM9NQkdpRS5Kd1Xl0R27sU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aloofzebra03/Ielektron-Internship/blob/main/Project/Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GUiJ52NDstZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff9fb7a8-2fcf-45e7-d86e-c4bbafaa179f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import Reshape"
      ],
      "metadata": {
        "id": "TXC5ebU2TmtX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TerminateOnNaN, CSVLogger"
      ],
      "metadata": {
        "id": "nru0w2zzsvM9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "class_map = {'person': 1, 'rider': 2 'motorcycle': 3, 'bicycle': 4, 'autorickshaw': 5, 'car': 6, 'truck': 7, 'bus': 8, 'train': 9, 'traffic light':10}\n",
        "\n",
        "# Specify the paths to your dataset\n",
        "train_dirs = ['/content/drive/MyDrive/IDD_lite/train/BLR-2018-03-22_17-39-26_2_frontFar', '/content/drive/MyDrive/IDD_lite/train/BLR-2018-03-22_17-39-26_3_frontFar'\n",
        "              ,'/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_15-24-27_frontFar','/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_15-44-27_frontFar'\n",
        "              ,'/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_15-54-27_frontFar','/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_16-04-27_frontFar'\n",
        "              ,'/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_16-14-27_frontFar']\n",
        "val_dirs = ['/content/drive/MyDrive/IDD_lite/test/BLR-2018-04-19_17-06-55_frontFar', '/content/drive/MyDrive/IDD_lite/test/BLR-2018-04-19_17-16-55_frontFar'\n",
        "              ,'/content/drive/MyDrive/IDD_lite/test/BLR-2018-04-19_17-26-55_frontFar']\n"
      ],
      "metadata": {
        "id": "P6_Jvhzntzcp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Helper function to parse XML files\n",
        "def parse_annotation(xml_file, class_map):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "    boxes = []\n",
        "    labels = []\n",
        "    for obj in root.findall('object'):\n",
        "        class_name = obj.find('name').text\n",
        "        if class_name in class_map:\n",
        "            class_id = class_map[class_name]\n",
        "            bbox = obj.find('bndbox')\n",
        "            xmin = int(bbox.find('xmin').text)\n",
        "            ymin = int(bbox.find('ymin').text)\n",
        "            xmax = int(bbox.find('xmax').text)\n",
        "            ymax = int(bbox.find('ymax').text)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])\n",
        "            labels.append(class_id)\n",
        "    return np.array(boxes), np.array(labels)\n",
        "\n",
        "# Function to find all JPEG and XML files in the given directory\n",
        "def find_files(directory, image_extension, annotation_extension):\n",
        "    image_files = []\n",
        "    annotation_files = []\n",
        "    for root, _, filenames in os.walk(directory):\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(image_extension):\n",
        "                image_files.append(os.path.join(root, filename))\n",
        "            elif filename.endswith(annotation_extension):\n",
        "                annotation_files.append(os.path.join(root, filename))\n",
        "    return image_files, annotation_files\n",
        "\n",
        "# Function to match image files with their corresponding XML annotation files\n",
        "def match_files(image_files, annotation_files):\n",
        "    image_to_annotation = {}\n",
        "    for image_file in image_files:\n",
        "        base_name = os.path.splitext(os.path.basename(image_file))[0]\n",
        "        for annotation_file in annotation_files:\n",
        "            if os.path.splitext(os.path.basename(annotation_file))[0] == base_name:\n",
        "                image_to_annotation[image_file] = annotation_file\n",
        "                break\n",
        "    return image_to_annotation\n",
        "\n",
        "# Function to load dataset with padding\n",
        "def load_dataset(dirs, class_map, max_boxes=100):\n",
        "    images = []\n",
        "    boxes_and_labels = []\n",
        "\n",
        "    for directory in dirs:\n",
        "        image_files, annotation_files = find_files(directory, '.jpg', '.xml')\n",
        "        image_to_annotation = match_files(image_files, annotation_files)\n",
        "        for image_file, annotation_file in image_to_annotation.items():\n",
        "            image = np.array(Image.open(image_file).resize((300, 300)))\n",
        "            box, label = parse_annotation(annotation_file, class_map)\n",
        "            if len(box) == 0:  # Skip images with no bounding boxes\n",
        "                continue\n",
        "            images.append(image)\n",
        "            padded_boxes = np.zeros((max_boxes, 4))\n",
        "            padded_labels = np.zeros(max_boxes)\n",
        "            if len(box) > 0:\n",
        "                padded_boxes[:len(box)] = box\n",
        "                padded_labels[:len(label)] = label\n",
        "            boxes_and_labels.append(np.concatenate([padded_boxes, padded_labels[:, None]], axis=-1))\n",
        "\n",
        "    return np.array(images), np.array(boxes_and_labels)\n",
        "\n",
        "def save_dataset(images, boxes_and_labels, file_path):\n",
        "    with open(file_path, 'wb') as f:\n",
        "        pickle.dump((images, boxes_and_labels), f)\n",
        "\n",
        "def load_dataset_from_file(file_path):\n",
        "    with open(file_path, 'rb') as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "# Paths to save/load the datasets\n",
        "train_dataset_file = '/content/drive/MyDrive/IDD_lite/train_dataset.pkl'\n",
        "val_dataset_file = '/content/drive/MyDrive/IDD_lite/val_dataset.pkl'\n",
        "\n",
        "# Load or save the training dataset\n",
        "if os.path.exists(train_dataset_file):\n",
        "    train_images, train_boxes_and_labels = load_dataset_from_file(train_dataset_file)\n",
        "else:\n",
        "    train_images, train_boxes_and_labels = load_dataset(train_dirs, class_map)\n",
        "    save_dataset(train_images, train_boxes_and_labels, train_dataset_file)\n",
        "print('done')\n",
        "# Load or save the validation dataset\n",
        "if os.path.exists(val_dataset_file):\n",
        "    val_images, val_boxes_and_labels = load_dataset_from_file(val_dataset_file)\n",
        "else:\n",
        "    val_images, val_boxes_and_labels = load_dataset(val_dirs, class_map)\n",
        "    save_dataset(val_images, val_boxes_and_labels, val_dataset_file)"
      ],
      "metadata": {
        "id": "0Hl3L4dQt1F0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520020b1-ecb5-411e-e9a0-7db277495913"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "pretrained_model_path = '/content/drive/MyDrive/IDD_lite/VGG_coco_SSD_300x300_iter_400000_subsampled_34_classes.h5'\n",
        "num_classes = 10  # Your custom number of classes\n",
        "input_shape = (300, 300, 3)"
      ],
      "metadata": {
        "id": "h6_9UwGOuVXt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the SSD300 model\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=input_shape, include_top=False)\n",
        "x = base_model.output"
      ],
      "metadata": {
        "id": "thUdVsBTuYPr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e378f571-555f-4ab5-fb15-d9d73e78602c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`input_shape` is undefined or non-square, or `rows` is not in [96, 128, 160, 192, 224]. Weights for input shape (224, 224) will be loaded as the default.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
            "9406464/9406464 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add additional SSD layers\n",
        "x = Conv2D(256, (3, 3), padding='same', activation='relu')(x)\n",
        "x = Conv2D(num_classes + 4, (1, 1))(x)\n",
        "x = Reshape((-1, num_classes + 4))(x)"
      ],
      "metadata": {
        "id": "db7GxDemvJye"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = x\n",
        "\n",
        "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# Load pre-trained weights\n",
        "model.load_weights(pretrained_model_path, by_name=True, skip_mismatch=True)\n"
      ],
      "metadata": {
        "id": "s8A0LjCUvNJ3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yh3MWTPGcki",
        "outputId": "318f98d7-dd57-4834-83a6-17aa7dc3dabb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1933, 300, 300, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_boxes_and_labels.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gts7t6nMGfXg",
        "outputId": "20cf56de-9e22-42c4-fae9-46b2151f4adc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1933, 20, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom loss function for SSD\n",
        "def smooth_l1_loss(y_true, y_pred):\n",
        "    diff = tf.abs(y_true - y_pred)\n",
        "    less_than_one = tf.cast(tf.less(diff, 1.0), tf.float32)\n",
        "    loss = less_than_one * 0.5 * tf.square(diff) + (1.0 - less_than_one) * (diff - 0.5)\n",
        "    return loss\n",
        "\n",
        "def ssd_loss(y_true, y_pred):\n",
        "    y_true_bboxes = y_true[:, :, :4]\n",
        "    y_true_labels = y_true[:, :, 4]\n",
        "    y_pred_bboxes = y_pred[:, :, :4]\n",
        "    y_pred_labels = y_pred[:, :, 4:]\n",
        "\n",
        "    # Calculate the localization loss (smooth L1 loss)\n",
        "    localization_loss = smooth_l1_loss(y_true_bboxes, y_pred_bboxes)\n",
        "    localization_loss = tf.reduce_sum(localization_loss, axis=-1)  # Reduce sum over the bounding box coordinates\n",
        "\n",
        "    # Calculate the confidence loss (categorical crossentropy)\n",
        "    confidence_loss = tf.keras.losses.sparse_categorical_crossentropy(y_true_labels, y_pred_labels)\n",
        "\n",
        "    return tf.reduce_mean(localization_loss + confidence_loss)\n",
        "\n",
        "\n",
        "# Compile the model with the correct optimizer\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=ssd_loss)\n",
        "\n",
        "# Define callbacks\n",
        "checkpoint = ModelCheckpoint('ssd300_finetuned.h5', monitor='val_loss', save_best_only=True, save_weights_only=True)\n",
        "learning_rate_scheduler = LearningRateScheduler(schedule=lambda epoch: 0.001 * 0.1**(epoch // 10))\n",
        "terminate_on_nan = TerminateOnNaN()\n",
        "csv_logger = CSVLogger('ssd300_training_log.csv')\n",
        "\n",
        "# Train the model\n",
        "batch_size = 32\n",
        "epochs = 20\n",
        "\n",
        "history = model.fit(\n",
        "    train_images, train_boxes_and_labels,\n",
        "    batch_size=batch_size,\n",
        "    epochs=epochs,\n",
        "    validation_data=(val_images, val_boxes_and_labels),\n",
        "    callbacks=[checkpoint, learning_rate_scheduler, terminate_on_nan, csv_logger]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 582
        },
        "id": "E3xG5mIJt3V8",
        "outputId": "635657b1-5f17-4363-a50c-3ed91f2a9de7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-17-2a783c9805d0>\", line 15, in ssd_loss  *\n        localization_loss = smooth_l1_loss(y_true_bboxes, y_pred_bboxes)\n    File \"<ipython-input-17-2a783c9805d0>\", line 3, in smooth_l1_loss  *\n        diff = tf.abs(y_true - y_pred)\n\n    ValueError: Dimensions must be equal, but are 20 and 100 for '{{node ssd_loss/sub}} = Sub[T=DT_FLOAT](ssd_loss/strided_slice, ssd_loss/strided_slice_2)' with input shapes: [?,20,4], [?,100,4].\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-2a783c9805d0>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     38\u001b[0m     \u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_boxes_and_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filertfwlgus.py\u001b[0m in \u001b[0;36mtf__ssd_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0my_pred_bboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0my_pred_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m                 \u001b[0mlocalization_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_bboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_bboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m                 \u001b[0mlocalization_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocalization_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mconfidence_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/__autograph_generated_filecw4v_fsh.py\u001b[0m in \u001b[0;36mtf__smooth_l1_loss\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      8\u001b[0m                 \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUndefinedReturnValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                 \u001b[0mdiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m                 \u001b[0mless_than_one\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mless\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mless_than_one\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mless_than_one\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiff\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"<ipython-input-17-2a783c9805d0>\", line 15, in ssd_loss  *\n        localization_loss = smooth_l1_loss(y_true_bboxes, y_pred_bboxes)\n    File \"<ipython-input-17-2a783c9805d0>\", line 3, in smooth_l1_loss  *\n        diff = tf.abs(y_true - y_pred)\n\n    ValueError: Dimensions must be equal, but are 20 and 100 for '{{node ssd_loss/sub}} = Sub[T=DT_FLOAT](ssd_loss/strided_slice, ssd_loss/strided_slice_2)' with input shapes: [?,20,4], [?,100,4].\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('/content/drive/MyDrive/IDD_lite/model_trained.h5')"
      ],
      "metadata": {
        "id": "ExAit0Fzt5cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making Predictions"
      ],
      "metadata": {
        "id": "E_sHTWSlW2Mk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Predictions"
      ],
      "metadata": {
        "id": "oWsjMeqrX9k-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "UwpifXB_YAJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_image(img_path, target_size=(300, 300)):\n",
        "    img = image.load_img(img_path, target_size=target_size)\n",
        "    img = image.img_to_array(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    return img\n"
      ],
      "metadata": {
        "id": "RAzG8FImWYGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_image(model, preprocessed_image):\n",
        "    predictions = model.predict(preprocessed_image)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "id": "nLx32pBbXFoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_predictions(predictions, class_map, confidence_threshold=0.5):\n",
        "    bboxes = predictions[..., :4]\n",
        "    scores = predictions[..., 4:]\n",
        "\n",
        "    # Get class labels with highest score\n",
        "    class_ids = np.argmax(scores, axis=-1)\n",
        "    confidences = np.max(scores, axis=-1)\n",
        "\n",
        "    decoded_boxes = []\n",
        "    for bbox, class_id, confidence in zip(bboxes[0], class_ids[0], confidences[0]):\n",
        "        if confidence > confidence_threshold:\n",
        "            class_name = [k for k, v in class_map.items() if v == class_id][0]\n",
        "            decoded_boxes.append((class_name, confidence, bbox))\n",
        "    return decoded_boxes\n",
        "\n"
      ],
      "metadata": {
        "id": "fq1WheLhXkv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_image_path = '/content/drive/MyDrive/IDD_lite/train/BLR-2018-04-16_16-14-27_frontFar/000054_r.jpg'"
      ],
      "metadata": {
        "id": "I1vm7PI3ZbM4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from google.colab.patches import cv2_imshow  # Specific to Google Colab\n",
        "\n",
        "def draw_boxes_cv(image_path, predictions, class_map, confidence_threshold=0.5):\n",
        "    # Load the image\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        raise ValueError(f\"Image at {image_path} could not be loaded.\")\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "    image_height, image_width, _ = image.shape\n",
        "\n",
        "    # Iterate over each prediction\n",
        "    for class_name, confidence, bbox in predictions:\n",
        "        if confidence > confidence_threshold:\n",
        "            # Convert bounding boxes from normalized to pixel values if necessary\n",
        "            xmin, ymin, xmax, ymax = bbox\n",
        "            if xmax <= 1.0 and ymax <= 1.0:  # Assuming normalized coordinates\n",
        "                xmin = int(xmin * image_width)\n",
        "                ymin = int(ymin * image_height)\n",
        "                xmax = int(xmax * image_width)\n",
        "                ymax = int(ymax * image_height)\n",
        "            else:\n",
        "                xmin, ymin, xmax, ymax = int(xmin), int(ymin), int(xmax), int(ymax)\n",
        "\n",
        "            # Draw the rectangle\n",
        "            cv2.rectangle(image, (xmin, ymin), (xmax, ymax), (255, 0, 0), 2)  # Blue color with thickness of 2\n",
        "\n",
        "            # Add label and confidence score\n",
        "            label = f'{class_name}: {confidence:.2f}'\n",
        "            label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)\n",
        "            label_ymin = max(ymin, label_size[1] + 10)\n",
        "            cv2.rectangle(image, (xmin, label_ymin - label_size[1] - 10), (xmin + label_size[0], label_ymin + 10), (255, 0, 0), cv2.FILLED)\n",
        "            cv2.putText(image, label, (xmin, label_ymin - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
        "\n",
        "    # Convert RGB to BGR for displaying with OpenCV\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
        "    cv2_imshow(image)  # Use cv2_imshow for Google Colab\n",
        "    cv2.waitKey(0)\n",
        "    cv2.destroyAllWindows()\n",
        "\n",
        "# Example usage\n",
        "preprocessed_image = preprocess_image(new_image_path)\n",
        "predictions = predict_image(model, preprocessed_image)\n",
        "decoded_predictions = decode_predictions(predictions, class_map)\n",
        "draw_boxes_cv(new_image_path, decoded_predictions, class_map)\n"
      ],
      "metadata": {
        "id": "hrj-tuNDXsuq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_cUL9iJqXzn2"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}